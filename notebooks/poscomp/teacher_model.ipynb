{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383baf73",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728dafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, RootModel\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override = True)\n",
    "\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a930c6",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "741e7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceSummary(BaseModel):\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    total_tokens: int\n",
    "    estimated_cost: float\n",
    "\n",
    "\n",
    "class Inference:\n",
    "    def __init__(\n",
    "        self, model: str, max_tokens: int, base_url: str, temperature: float = 0.4, stream: bool = True\n",
    "    ):\n",
    "        self.openai = OpenAI(api_key=os.getenv(\"DEEPINFRA_API_KEY\"), base_url=base_url)\n",
    "\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.stream = stream\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float = None,\n",
    "        stream: bool = None,\n",
    "        max_tokens: int = None,\n",
    "        model: str = None,\n",
    "        verbose: int = 1,\n",
    "    ) -> tuple[str, InferenceSummary]:\n",
    "        temperature = temperature or self.temperature\n",
    "        stream = stream or self.stream\n",
    "        max_tokens = max_tokens or self.max_tokens\n",
    "        model = model or self.model\n",
    "\n",
    "        chunks = self.openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            stream=stream,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        \n",
    "        output = \"\"\n",
    "        usage = None\n",
    "        if stream:\n",
    "            for chunk in chunks:\n",
    "                delta = chunk.choices[0].delta\n",
    "\n",
    "                if delta and delta.content:\n",
    "                    if verbose > 0:\n",
    "                        print(delta.content, end=\"\", flush=True)\n",
    "\n",
    "                    output += delta.content\n",
    "\n",
    "                if chunk.choices[0].finish_reason == \"stop\":\n",
    "                    usage = chunk.usage\n",
    "        else:\n",
    "            output = chunks.choices[0].message.content\n",
    "            usage = chunks.usage\n",
    "\n",
    "        summary = InferenceSummary(\n",
    "            input_tokens=usage.prompt_tokens,\n",
    "            output_tokens=usage.completion_tokens,\n",
    "            total_tokens=usage.total_tokens,\n",
    "            estimated_cost=usage.estimated_cost,\n",
    "        )\n",
    "        return output, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "247366a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    id: str\n",
    "    edicao: int\n",
    "    numero: int\n",
    "    enunciado: str\n",
    "    alternativas: list[str]\n",
    "    area_conhecimento: str\n",
    "    area: str\n",
    "    subarea: str\n",
    "    dificuldade: str\n",
    "    gabarito: str\n",
    "    solucao: str\n",
    "    dificuldade_experimental: Optional[str] = None\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    id: str\n",
    "    statement: str\n",
    "    alternatives: list[str]\n",
    "    area_knowledge: str\n",
    "    area: str\n",
    "    subarea: str\n",
    "    answer: str\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "def load_questions(input_path: str) -> list[Question]:\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_questions: str = json.load(f)\n",
    "\n",
    "        questions: list[Question] = [\n",
    "            Question.model_validate_json(json.dumps(raw_question))\n",
    "            for raw_question in raw_questions\n",
    "        ]\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_questions(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    model: str,\n",
    "    base_url: str,\n",
    "    system_prompt: str,\n",
    "    max_tokens: int,\n",
    "    stream: bool,\n",
    "    verbose: int = 1,\n",
    "):\n",
    "    questions = load_questions(input_path=input_path)\n",
    "\n",
    "    inference_model = Inference(\n",
    "        model=model, \n",
    "        max_tokens=max_tokens, \n",
    "        stream=stream, \n",
    "        base_url=base_url\n",
    "    )\n",
    "\n",
    "    total_execution_time = 0\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    total_tokens = 0\n",
    "    total_estimated_cost = 0\n",
    "\n",
    "    responses: list[Response] = []\n",
    "\n",
    "    def process_single_question(question: Question) -> tuple[Response, InferenceSummary, float]:\n",
    "        user_prompt = (\n",
    "            f\"Quest√£o:\\n{question.enunciado}\\nAlternativas:\\n\"\n",
    "            + \"\\n\".join(question.alternativas)\n",
    "            + f\"\\nGabarito: {question.gabarito}\"\n",
    "        )\n",
    "\n",
    "        response = Response(\n",
    "            id=question.id,\n",
    "            statement=question.enunciado,\n",
    "            alternatives=question.alternativas,\n",
    "            area_knowledge=question.area_conhecimento,\n",
    "            area=question.area,\n",
    "            subarea=question.subarea,\n",
    "            answer=question.gabarito,\n",
    "            reasoning=\"\",\n",
    "        )\n",
    "\n",
    "        max_retries = 5\n",
    "        retry_delay = 1\n",
    "        attempt = 0\n",
    "\n",
    "        while attempt < max_retries:\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                output, summary = inference_model.execute(\n",
    "                    system_prompt=system_prompt, user_prompt=user_prompt\n",
    "                )\n",
    "                response.reasoning = output\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                return response, summary, elapsed\n",
    "            \n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                wait = retry_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "                if attempt == max_retries:\n",
    "                    summary = InferenceSummary(\n",
    "                        input_tokens=0,\n",
    "                        output_tokens=0,\n",
    "                        total_tokens=0,\n",
    "                        estimated_cost=0.0,\n",
    "                    )\n",
    "                    response.reasoning = f\"[Error after  {max_retries} retries] {str(e)}\"\n",
    "                    elapsed = time.time() - start_time\n",
    "\n",
    "                    return response, summary, elapsed\n",
    "                \n",
    "                else:\n",
    "                    print(f\"[Attempt {attempt}/{max_retries}] Error: {e}. Trying again in {wait:.2f} secs...\")\n",
    "                    time.sleep(wait)\n",
    "\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(process_single_question, q) for q in questions]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            response, summary, elapsed = future.result()\n",
    "            responses.append(response)\n",
    "\n",
    "            total_execution_time += elapsed\n",
    "            total_input_tokens += summary.input_tokens\n",
    "            total_output_tokens += summary.output_tokens\n",
    "            total_tokens += summary.total_tokens\n",
    "            total_estimated_cost += summary.estimated_cost\n",
    "\n",
    "            if verbose == 2:\n",
    "                print(f\"\\n\\n\\n === SUMMARY ===\", flush=True)\n",
    "                print(f\"Execution time: {elapsed:.2f}s\", flush=True)\n",
    "                print(f\"Input tokens: {summary.input_tokens}\", flush=True)\n",
    "                print(f\"Output tokens: {summary.output_tokens}\", flush=True)\n",
    "                print(f\"Total tokens: {summary.total_tokens}\", flush=True)\n",
    "                print(f\"Estimated cost: ${summary.estimated_cost:.6f}\", flush=True)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Total execution time: {total_execution_time:.2f}s\")\n",
    "        print(f\"Total input tokens: {total_input_tokens}\")\n",
    "        print(f\"Total output tokens: {total_output_tokens}\")\n",
    "        print(f\"Total tokens: {total_tokens}\")\n",
    "        print(f\"Total estimated cost: ${total_estimated_cost:.6f}\")\n",
    "\n",
    "    Responses = RootModel[list[Response]]\n",
    "    json_responses = Responses(responses).model_dump_json(indent=2)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e81c7b5",
   "metadata": {},
   "source": [
    "# Single Teacher Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c2e68",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7e381507",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"data/sample.json\"\n",
    "OUTPUT_PATH = \"data/output.json\"\n",
    "\n",
    "BASE_URL = \"https://api.deepinfra.com/v1/openai\"\n",
    "\n",
    "TEACHER_MODEL = \"deepseek-ai/DeepSeek-R1-Turbo\"\n",
    "MAX_TOKENS = 10000\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "      \"Voc√™ √© um professor experiente em ensino de Ci√™ncia da Computa√ß√£o, com foco em ajudar estudantes a entender as quest√µes do POSCOMP.\\n\\n\"\n",
    "\n",
    "      \"Objetivo:\\n\"\n",
    "      \"Analisar e resolver cada quest√£o passo a passo, explicando detalhadamente cada decis√£o tomada no processo.\\n\\n\"\n",
    "\n",
    "      \"Princ√≠pios orientadores:\\n\"\n",
    "      \"- Simule seu ‚Äúprocesso de pensamento‚Äù, explicando cada etapa e racioc√≠nio.\\n\"\n",
    "      \"- Explique todos os conceitos e teorias aplicadas.\\n\"\n",
    "      \"- Em quest√µes de programa√ß√£o, destaque a l√≥gica por tr√°s do algoritmo.\\n\"\n",
    "      \"- Em quest√µes matem√°ticas, mostre os c√°lculos claramente, passo a passo.\\n\"\n",
    "      \"- Evite repetir o texto da quest√£o ou suas alternativas.\\n\"\n",
    "      \"- Reforce a explica√ß√£o com pseudoc√≥digo, f√≥rmulas ou diagramas, se necess√°rio.\\n\"\n",
    "      \"- Use uma linguagem simples e clara, como em uma tutoria particular para alunos brasileiros.\\n\"\n",
    "      \"- Finalize com: RESPOSTA FINAL: (letra).\\n\\n\"\n",
    "      \"- **NUNCA** use outro idioma que n√£o seja o portugu√™s em suas respostas.\\n\"\n",
    "\n",
    "      \"Objetivo: Garantir que o aluno compreenda o conte√∫do, e n√£o apenas memorize respostas.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_questions(\n",
    "    input_path=INPUT_PATH,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    model=TEACHER_MODEL,\n",
    "    base_url=BASE_URL,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    stream=False,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef576b",
   "metadata": {},
   "source": [
    "# Multiple Teacher Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f4794",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"data/sample.json\"\n",
    "SYSTEM_PROMPTS_PATH = \"data/prompts.json\"\n",
    "OUTPUT_DIR = \"data/results\"\n",
    "\n",
    "BASE_URL = \"https://api.deepinfra.com/v1/openai\"\n",
    "\n",
    "TEACHER_MODELS = [\n",
    "    \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Turbo\",\n",
    "    \"microsoft/phi-4\",\n",
    "    \"Qwen/QwQ-32B\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    \"deepseek-ai/DeepSeek-Prover-V2-671B\",\n",
    "    \"deepseek-ai/DeepSeek-V3-0324\",\n",
    "    \"Qwen/Qwen3-235B-A22B\",\n",
    "]\n",
    "MAX_TOKENS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SYSTEM_PROMPTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    system_prompts = json.load(f)\n",
    "    \n",
    "for model in TEACHER_MODELS:\n",
    "    print(f\"\\n\\n\\n{'#'*30}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    \n",
    "    process_questions(\n",
    "        input_path=INPUT_PATH,\n",
    "        output_path=f\"{OUTPUT_DIR}/output-{model.split('/')[1]}.json\",\n",
    "        model=model,\n",
    "        base_url=BASE_URL,\n",
    "        system_prompt=system_prompts[model][\"prompt\"],\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        stream=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
