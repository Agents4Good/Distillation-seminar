<h1 align="center">Semin√°rio Destila√ß√£o - Artefatos</h1>

> Este reposit√≥rio documenta o conte√∫do e os artefatos utilizados no semin√°rio apresentado sobre **Destila√ß√£o de Conhecimento**.

---
## üìö Artigos Utilizados

1. **[Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)**  
   Artigo inovador que introduz a t√©cnica de *knowledge distillation* com redes professor-aluno.

2. **[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)**    
   Proposta de destila√ß√£o de modelos BERT com t√©cnicas de compress√£o em v√°rios n√≠veis.

3. **[A Survey on Knowledge Distillation of Large Language Models](https://arxiv.org/html/2402.13116v4)**   
   Pesquisa completa sobre Destila√ß√£o em LLMs (2024).

---
## üõ†Ô∏è Artefatos

- **Apresenta√ß√£o em slides**: [Slides/](...)
- **Resumo dos artigos**: [MasterChef-AI](https://agents4good.github.io/MasterChef-AI/content/destilacao/)
- **C√≥digo de exemplo**: [POSCOMP/](POSCOMP/)
