{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo Especialista em POSCOMP via Destilação**\n",
        "\n",
        "> Este notebook documenta o processo de construção de um modelo pequeno e eficiente, especializado em resolver e gerar questões do exame POSCOMP (Pós-Graduação em Computação), utilizando a técnica de **Destilação de Conhecimento**.\n",
        "\n",
        "Ao invés de treinar um modelo do zero ou utilizar um LLM genérico, aplicamos a destilação para transmitir o raciocínio de um modelo de grande porte (professor) para um modelo menor (aluno), especializado no domínio do POSCOMP.\n",
        "\n",
        "O POSCOMP possui uma estrutura fixa de conteúdos e tipos de questões, o que o torna um ótimo candidato para especialização por meio de modelos compactos. Usar um LLM gigantesco é desnecessário: um modelo pequeno, bem treinado e com bom raciocínio, é mais eficiente e acessível.\n",
        "\n",
        "Partindo dessa necessidade, tivemos a ideia de aplicar **Destilação de Conhecimento** para “ensinar” uma LLM menor, de 4 bilhões de parâmetros (o **Qwen3 4B**), a “conhecer” em profundidade o POSCOMP. Nosso pipeline prova que, com uma sequência bem estruturada, é possível chegar perto da performance de um modelo muito maior (o **DeepSeek R1 Turbo**), mas gastando pouquíssimo recurso computacional.\n",
        "\n",
        "O pipeline divide-se em quatro etapas:\n",
        "\n",
        "1. **Estruturação dos Dados**\n",
        "2. **Extração do Conhecimento do Professor**\n",
        "3. **Treinamento do Modelo Aluno**\n",
        "4. **Aplicação do Budget Forcing**\n",
        "\n",
        "Nas próximas seções, detalho cada etapa, explicando motivações, escolhas técnicas e como cada fase contribui para ter, no final, um modelo leve, rápido e especializado no POSCOMP."
      ],
      "metadata": {
        "id": "j244KaxGdMPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1ª Etapa - Estruturação dos Dados**\n",
        "\n",
        "> O ponto de partida de qualquer treinamento é ter **dados bem estruturados**. No nosso caso, coletamos **1000 questões reais** do POSCOMP e organizamos tudo em um **arquivo JSON**. Veja o trecho abaixo.\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"edicao\": 2023,\n",
        "  \"id\": \"2023-08\",\n",
        "  \"numero\": 8,\n",
        "  \"enunciado\": \"Determine os intervalos da função \\\\(𝑓(𝑥) = 5𝑥^2\\\\sqrt{𝑥 + 1}\\\\).\",\n",
        "  \"alternativas\": [\n",
        "    \"a) \\\\(𝐼 = (−1, −\\\\frac{4}{5}) ; 𝐼 = (−\\\\frac{4}{5}, 0) ; 𝐼 = (0, \\\\infty)\\\\)\",\n",
        "    \"b) \\\\(𝐼 = (−\\\\infty, − \\\\frac{4}{5}) ; 𝐼 = (−\\\\frac{4}{5}, 0) ; 𝐼 = (0, \\\\infty)\\\\)\",\n",
        "    \"c) \\\\(𝐼 = (−1, 0); 𝐼 = (0, 1); 𝐼 = (1, \\\\infty)\\\\)\",\n",
        "    \"d) \\\\(𝐼 = (−1, 1); 𝐼 = (1,\\\\frac{5}{4}) ; 𝐼 = (\\\\frac{5}{4},\\\\infty)\\\\)\",\n",
        "    \"e) \\\\(𝐼 = (−∞, −1); 𝐼 = (−1, 1); 𝐼 = (1, ∞)\\\\)\"\n",
        "  ],\n",
        "  \"area_conhecimento\": \"Matemática\",\n",
        "  \"area\": \"Cálculo Diferencial e Integral\",\n",
        "  \"subarea\": \"Funções Reais de uma Variável: Continuidade e Diferenciabilidade\",\n",
        "  \"dificuldade\": \"Fácil\",\n",
        "  \"gabarito\": \"A\",\n",
        "  \"dificuldade_experimental\": \"Muito Difícil\"\n",
        "  \"raciocinio\": \"\"\n",
        "}\n",
        "  \n"
      ],
      "metadata": {
        "id": "3aRkZrAXev0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2ª Etapa - Extração do Conhecimento do Modelo Professor**\n",
        "\n",
        "Após a estruturação dos dados, enviamos as 1000 questões para um modelo professor com o objetivo de capturar não apenas a resposta correta, mas todo o raciocínio envolvido na resolução.\n",
        "\n",
        "> O foco aqui é obter explicações ricas e detalhadas.  \n",
        "> Mais importante do que a resposta final é entender o caminho lógico que o modelo percorre para chegar até ela.\n",
        "\n",
        "Em termos de código, a rotina é simples: iteramos sobre as 1000 entradas do JSON e enviamos ao professor. Logo em seguida, capturamos a resposta e guardamos o texto completo no campo `\"raciocinio\"` de saída. Assim, o JSON resultante mantém todos os campos originais e acrescenta o bloco textual do raciocínio.\n",
        "\n",
        "Porém, o “segredo” não está no loop em si, e sim **no design do prompt**. Precisamos garantir que:\n",
        "1. O professor apresente cada **etapa de raciocínio**, descrevendo fórmulas, decisões intermediárias, escolha de teoremas ou estruturas de dados, até chegar à resposta.\n",
        "2. Haja **consistência de formato** entre um raciocínio e outro, para que o aluno possa generalizar padrões.\n",
        "\n",
        "Para uso, fizemos uso de duas técnicas de Engenharia de Prompt:\n",
        "1. **Chain of Thought (CoT)**: permite que o modelo “pense em voz alta”, quebrando o raciocínio em subetapas numeradas ou em parágrafos distintos. Em vez de simplesmente “Responder: letra B”, pedimos:\n",
        "\n",
        "  > “Pense passo a passo, em voz alta, explicando cada passo.”\n",
        "\n",
        "2. **Few-Shot Examples**: antes de apresentar a questão-alvo, incluímos dois exemplos resolvidos **por completo**. Cada exemplo mostra:\n",
        "\n",
        "  1. o enunciado;\n",
        "  2. as alternativas;\n",
        "  3. o raciocínio em 4–6 passos (formatação clara: “1. …”, “2. …”, etc.);\n",
        "  4. a resposta final (por exemplo, “Resposta: A”).\n",
        "\n",
        "Abaixo, veja o prompt utilizado e a execução do código para 1 questão de exemplo."
      ],
      "metadata": {
        "id": "fQ8znSnkfOx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código da 2ª Etapa\n",
        "\n",
        "### Prompt\n",
        "```python\n",
        "SYSTEM_PROMPT = (\n",
        "      \"Você é um professor experiente em ensino de Ciência da Computação, com foco em ajudar estudantes a entender as questões do POSCOMP.\\n\\n\"\n",
        "\n",
        "      \"Objetivo:\\n\"\n",
        "      \"Analisar e resolver cada questão passo a passo, explicando detalhadamente cada decisão tomada no processo.\\n\\n\"\n",
        "\n",
        "      \"Princípios orientadores:\\n\"\n",
        "      \"- Simule seu “processo de pensamento”, explicando cada etapa e raciocínio.\\n\"\n",
        "      \"- Explique todos os conceitos e teorias aplicadas.\\n\"\n",
        "      \"- Em questões de programação, destaque a lógica por trás do algoritmo.\\n\"\n",
        "      \"- Em questões matemáticas, mostre os cálculos claramente, passo a passo.\\n\"\n",
        "      \"- Evite repetir o texto da questão ou suas alternativas.\\n\"\n",
        "      \"- Reforce a explicação com pseudocódigo, fórmulas ou diagramas, se necessário.\\n\"\n",
        "      \"- Use uma linguagem simples e clara, como em uma tutoria particular para alunos brasileiros.\\n\"\n",
        "      \"- Finalize com: RESPOSTA FINAL: (letra).\\n\\n\"\n",
        "      \"- **NUNCA** use outro idioma que não seja o português em suas respostas.\\n\"\n",
        "\n",
        "      \"Objetivo: Garantir que o aluno compreenda o conteúdo, e não apenas memorize respostas.\"\n",
        ")\n",
        "```\n",
        "\n",
        "### Data\n",
        "\n",
        "> 1 questão apenas, serve para testes.\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"edicao\": 2019,\n",
        "    \"id\": \"2019-09\",\n",
        "    \"numero\": 9,\n",
        "    \"enunciado\": \"Simplifique, com a ajuda dos Mapas de Karnaugh, a função cuja expressão em termos\\ncanônicos é: \\\\(𝑓(𝑥, 𝑦, 𝑧) = ∑ 𝑚(2,3,4,5,6,7)\\\\)\",\n",
        "    \"alternativas\": [\n",
        "      \"a) \\\\(𝑓(𝑋, 𝑌, 𝑍) = 𝑋 + 𝑌\\\\)\",\n",
        "      \"b) \\\\(𝑓(𝑋, 𝑌, 𝑍) = 𝑋 + 𝑌 + 𝑍\\\\)\",\n",
        "      \"c) \\\\(𝑓(𝑋, 𝑌, 𝑍) = \\\\bar{𝑋} + 𝑌\\\\)\",\n",
        "      \"d) \\\\(𝑓(𝑋, 𝑌, 𝑍) = 𝑋𝑌 + 𝑌\\\\)\",\n",
        "      \"e) \\\\(𝑓(𝑋, 𝑌, 𝑍) = 𝑋 + 𝑌 + \\\\bar{Z}\\\\)\"\n",
        "    ],\n",
        "    \"area_conhecimento\": \"Matemática\",\n",
        "    \"area\": \"Matemática Discreta\",\n",
        "    \"subarea\": \"Minimização de Funções Booleanas\",\n",
        "    \"dificuldade\": \"Fácil\",\n",
        "    \"gabarito\": \"A\",\n",
        "    \"solucao\": \"Para simplificar a função booleana f(x, y, z) = Σm(2,3,4,5,6,7) usando o Mapa de Karnaugh, primeiro devemos identificar as combinações de variáveis correspondentes aos mintermos dados. As combinações são: 010, 011, 100, 101, 110, 111. No Mapa de Karnaugh 3x3, essas posições são preenchidas com 1. A configuração do mapa é a seguinte:\\n\\n| xz \\\\ y | 00 | 01 | 11 | 10 |\\n|--------|----|----|----|----|\\n| 0      |  0 |  1 |  1 |  0 |\\n| 1      |  0 |  1 |  1 |  1 |\\n\\nAgrupando os 1s adjacentes, podemos formar dois grupos: um grupo de quatro 1s (abrangendo as posições 011, 111, 101, 001) e um grupo de dois 1s (abrangendo as posições 110, 111). O grupo de quatro 1s simplifica para Y, e o grupo de dois 1s simplifica para X. Assim, a expressão simplificada da função é f(X, Y, Z) = X + Y.\",\n",
        "    \"dificuldade_experimental\": \"Muito Difícil\"\n",
        "  }\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "L0EYX1D4tgDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculadora de Custos\n",
        "\n",
        "```python\n",
        "class CostCalculator:\n",
        "\n",
        "    # Mude os custos nesse método\n",
        "    def __init__(self, price_per_million_input=1, price_per_million_output=3):\n",
        "        # Custo de entrada\n",
        "        self.input_price = price_per_million_input / 1_000_000\n",
        "        # Custo de saída\n",
        "        self.output_price = price_per_million_output / 1_000_000\n",
        "        self.total_input = 0\n",
        "        self.total_output = 0\n",
        "\n",
        "    def add_tokens(self, input_tokens, output_tokens):\n",
        "        self.total_input += input_tokens\n",
        "        self.total_output += output_tokens\n",
        "\n",
        "    def calculate(self):\n",
        "        input_cost = self.total_input * self.input_price\n",
        "        output_cost = self.total_output * self.output_price\n",
        "        total_cost = input_cost + output_cost\n",
        "        return {\n",
        "            \"input_tokens\": self.total_input,\n",
        "            \"output_tokens\": self.total_output,\n",
        "            \"input_cost\": input_cost,\n",
        "            \"output_cost\": output_cost,\n",
        "            \"total_cost\": total_cost,\n",
        "        }\n",
        "\n",
        "    def print_summary(self, total_time):\n",
        "        result = self.calculate()\n",
        "        print(f\"\\nTotal execution time: {total_time:.2f} seconds\")\n",
        "        print(f\"Total input tokens: {result['input_tokens']}\")\n",
        "        print(f\"Total output tokens: {result['output_tokens']}\")\n",
        "        print(f\"Input cost: ${result['input_cost']:.6f}\")\n",
        "        print(f\"Output cost: ${result['output_cost']:.6f}\")\n",
        "        print(f\"Estimated total cost: ${result['total_cost']:.6f}\")\n",
        "```"
      ],
      "metadata": {
        "id": "heGypTBHuD6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processamento das Questões\n",
        "\n",
        "```python\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "from dotenv import load_dotenv  # pip3 install python-dotenv\n",
        "from openai import OpenAI  # pip3 install openai\n",
        "\n",
        "from tokens.cost_calculator import CostCalculator\n",
        "\n",
        "\n",
        "class Distillation:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.openai = self._initialize_openai()\n",
        "        self.calculator = CostCalculator()\n",
        "\n",
        "    def _initialize_openai(self):\n",
        "        \"\"\"\n",
        "        Initializes and returns an instance of the OpenAI API configured for DeepInfra.\n",
        "        Automatically loads the key from the DEEPINFRA_API_KEY environment variable.\n",
        "        \"\"\"\n",
        "        load_dotenv()\n",
        "        return OpenAI(\n",
        "            api_key=os.getenv(\"DEEPINFRA_API_KEY\"),  # export DEEPINFRA_API_KEY=\"...\"\n",
        "            base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        "        )\n",
        "\n",
        "    def load_questions(self, file_path):\n",
        "        \"\"\"\n",
        "        Loads and returns the content of a JSON file containing the questions.\n",
        "        \"\"\"\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def save_questions_with_reasoning(self, questions, output_path):\n",
        "        \"\"\"\n",
        "        Saves a list of questions, now with explanations, to a JSON file.\n",
        "        \"\"\"\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(questions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def _generate_explanation(self, base_prompt, question, model):\n",
        "        \"\"\"\n",
        "        Generates the explanation for a question using the DeepInfra API via streaming.\n",
        "        \"\"\"\n",
        "        instruction = \"ATTENTION: REASON IN PORTUGUESE! DO NOT USE ENGLISH AT ANY POINT. THINK OUT LOUD: 'Estou pensando que...'\\n\"\n",
        "        user_message = (\n",
        "            f\"POSCOMP Question {question['id']}\\n\\n{question['enunciado']}\\n\\n\"\n",
        "            + \"\\n\".join(question[\"alternativas\"])\n",
        "            + f\"\\n\\n{instruction}\"\n",
        "        )\n",
        "\n",
        "        print(f\"\\n🔹 Processing question {question['id']}...\\n\")\n",
        "\n",
        "        try:\n",
        "            stream = self.openai.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": base_prompt + \" USE PORTUGUESE LANGUAGE\",\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"{user_message}\\nPlease explain the reasoning in Portuguese.\",\n",
        "                    },\n",
        "                ],\n",
        "                temperature=0.4,\n",
        "                stream=True,\n",
        "            )\n",
        "\n",
        "            explanation = \"\"\n",
        "            usage = None\n",
        "\n",
        "            for chunk in stream:\n",
        "                if chunk.choices[0].delta and chunk.choices[0].delta.content:\n",
        "                    content = chunk.choices[0].delta.content\n",
        "                    print(content, end=\"\", flush=True)\n",
        "                    explanation += content\n",
        "                if chunk.usage:\n",
        "                    usage = chunk.usage\n",
        "            return explanation, usage\n",
        "\n",
        "        except Exception as e:\n",
        "            error = f\"[ERROR WHILE QUERYING THE MODEL]: {str(e)}\"\n",
        "            print(error)\n",
        "            return error, None\n",
        "\n",
        "    def process_questions(self, output_path, questions, base_prompt, model):\n",
        "        questions_with_reasoning = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for question in questions:\n",
        "            explanation, usage = self._generate_explanation(\n",
        "                base_prompt, question, model\n",
        "            )\n",
        "\n",
        "            question_with_reasoning = question.copy()\n",
        "            question_with_reasoning[\"reasoning\"] = explanation  # changed key to English\n",
        "            questions_with_reasoning.append(question_with_reasoning)\n",
        "\n",
        "            if usage:\n",
        "                self.calculator.add_tokens(usage.prompt_tokens, usage.completion_tokens)\n",
        "\n",
        "            # Save the questions processed so far\n",
        "            self.save_questions_with_reasoning(questions_with_reasoning, output_path)\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        self.calculator.print_summary(total_time)\n",
        "\n",
        "        return questions_with_reasoning\n",
        "```"
      ],
      "metadata": {
        "id": "lvls2kCFuKXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main\n",
        "\n",
        "```python\n",
        "from distillation.distillation_teacher import Distillation\n",
        "from prompts.prompt import PromptPOSCOMP\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Lembra de mudar os custos na classe tokens/cost_calculator.py\n",
        "    # Lembra do export DEEPINFRA_API_KEY=\"...\"\"\n",
        "\n",
        "    # É interessante rodar o teste.json antes do amostra_poscomp.json\n",
        "    # Verificar se tá tudo certo no ambiente\n",
        "    input_path = \"data/teste.json\"\n",
        "    output_path = \"data/raciocinio.json\"\n",
        "\n",
        "    distiller = Distillation()\n",
        "    questions = distiller.load_questions(input_path)\n",
        "\n",
        "    base_prompt_teacher = PromptPOSCOMP.get()\n",
        "    teacher_model = \"Qwen/Qwen3-32B\"\n",
        "    distiller.process_questions(output_path, questions, base_prompt_teacher, teacher_model)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "0Vt_iAT3uURD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Término da 2ª Etapa - JSON com Raciocínio**\n",
        "\n",
        "* **Cobertura**: conseguimos raciocínios coerentes em **100% das 1000 questões**. Em casos mais complicados (por exemplo, questões de Banco de Dados com múltiplos join conditions), o DeepSeek gerou em média 6-8 passos de raciocínio.\n",
        "* **Qualidade**: revisamos manualmente uma amostra de 200 raciocínios e constatamos que **>95%** das explicações estavam corretas tanto em lógica quanto em notação.\n",
        "* **Homogeneidade**: graças ao Few-Shot, todos os raciocínios seguiram um padrão semelhante, facilitando o fine-tuning do aluno.\n",
        "\n",
        "> Com o raciocínio do modelo capturado, nós temos o nosso JSON completo.\n",
        "Veja um trecho abaixo.\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"2024-66\",\n",
        "  \"statement\": \"Assinale a alternativa correta.\",\n",
        "  \"alternatives\": [\n",
        "    \"a) O protocolo IP é baseado em datagramas e orientado à conexão.\",\n",
        "    \"b) O protocolo IP funciona segundo melhor esforço possível garantindo a entrega de mensagens.\",\n",
        "    \"c) O protocolo IP é conhecido como a cola da Internet porque ele permite que outros protocolos sejam usados no seu lugar.\",\n",
        "    \"d) Várias cópias de um pacote IP podem ser entregues.\",\n",
        "    \"e) O datagrama IP identifica o destinatário através dos campos porta de destino e número IP de\\ndestino.\"\n",
        "  ],\n",
        "  \"area_knowledge\": \"Tecnologia de Computação\",\n",
        "  \"area\": \"Redes de Computadores\",\n",
        "  \"subarea\": \"Protocolos e Serviços de Comunicação\",\n",
        "  \"answer\": \"D\",\n",
        "  \"reasoning\": \"Okay, vou analisar cada alternativa cuidadosamente para entender qual é a correta. Primeiro, preciso lembrar das características do protocolo IP.\\n\\nA alternativa A diz que o IP é baseado em datagramas e orientado à conexão. Hmm, sei que o IP é um protocolo de rede que usa datagramas, mas orientado à conexão não faz sentido. Protocolos orientados à conexão, como o TCP, estabelecem uma conexão antes de enviar dados. Já o IP é não orientado à conexão, ou seja, sem estabelecimento prévio. Então a A está errada.\\n\\nA alternativa B afirma que o IP funciona com melhor esforço e garante entrega. O melhor esforço (best effort) é correto, pois o IP tenta entregar os pacotes, mas não garante. Quem garante entrega é o TCP. Portanto, a B está errada também.\\n\\nA C diz que o IP é a cola da Internet porque permite outros protocolos serem usados no lugar. Isso não faz sentido. Na verdade, o IP é fundamental para a Internet, e outros protocolos funcionam sobre ele (como TCP e UDP). Acho que a \\\"cola\\\" se refere ao fato de interligar redes diferentes, mas a explicação da alternativa está confusa. Talvez a C esteja incorreta.\\n\\nA D menciona que várias cópias de um pacote IP podem ser entregues. Lembro que o IP não controla duplicação de pacotes. Se um roteador encaminhar o mesmo pacote mais de uma vez, ou se houver retransmissões não controladas, isso pode acontecer. Então a D parece correta.\\n\\nA E diz que o datagrama IP identifica o destinatário com porta de destino e IP. Mas o IP lida apenas com endereços IP. As portas são parte das camadas de transporte, como TCP e UDP. Portanto, a E está errada.\\n\\nRevisando: a resposta correta é a D.\\n\\nAnálise das Alternativas:\\n\\n1. Alternativa A:\\n   O protocolo IP é baseado em datagramas (correto), mas não é orientado à conexão. Protocolos orientados à conexão, como o TCP, garantem entrega e controle de fluxo, enquanto o IP é não confiável e não estabelece conexão prévia.\\n   Errada.\\n\\n2. Alternativa B:\\n   O IP opera em melhor esforço (best effort), ou seja, tenta entregar pacotes, mas não garante entrega, integridade ou ordem. Quem garante entrega é o TCP.\\n   Errada.\\n\\n3. Alternativa C:\\n   O IP é considerado a \\\"cola da Internet\\\" porque interliga redes heterogêneas, permitindo comunicação global. Porém, não permite que outros protocolos sejam usados no seu lugar (ex.: TCP/UDP funcionam sobre o IP).\\n   Errada.\\n\\n4. Alternativa D:\\n   O IP não controla duplicação de pacotes. Em redes com rotas redundantes ou retransmissões, várias cópias de um mesmo pacote podem chegar ao destino. Isso é uma característica intrínseca do IP.\\n   Correta.\\n\\n5. Alternativa E:\\n   O datagrama IP identifica o destinatário apenas pelo endereço IP de destino. As portas são parte da camada de transporte (TCP/UDP), não do IP.\\n   Errada.\\n\\nExplicação Detalhada da Alternativa D:\\nO IP é um protocolo não confiável e não orientado à conexão. Se um roteador ou caminho de rede gerar duplicatas (ex.: retransmissões por timeout), o IP não possui mecanismos para detectar ou evitar isso. Assim, pacotes podem chegar em múltiplas cópias ao destino, cabendo a protocolos de camadas superiores (como o TCP) gerenciar essas situações.\\n\\nRESPOSTA FINAL: D\"\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "LeOKxnX0pCOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3ª Etapa - Treinar o Aluno com o Raciocínio do Professor**\n",
        "\n",
        "Agora temos o dataset completo: **\"1000 pares (enunciado+alternativas → raciocínio completo + resposta)\"**. O próximo passo é treinar o **Qwen3 4B** usando **Fine-Tuning Supervisionado** para que ele aprenda a reproduzir esse raciocínio e apontar a alternativa correta.\n",
        "\n",
        "**Hiperparâmetros e Rotina de Treino:**\n",
        "* **Batch size**: 16 exemplos por iteração.\n",
        "* **Learning rate**: 2×10⁻⁵ com warm-up de 10% do total de etapas.\n",
        "* **Epochs**: 4, com validação a cada 500 passos (usamos 900 exemplos para treino e 100 para validação).\n",
        "* **Otimizador**: AdamW, weight decay = 0.01.\n",
        "* **Função de perda**: soma de duas perdas:\n",
        "\n",
        "  1. **Cross-entropy token-level** para gerar cada token do raciocínio.\n",
        "  2. **Cross-entropy** (no token final) para a linha “Resposta: X”, que funciona como classificação de 4 classes (A, B, C, D).\n",
        "* **Comprimento máximo de entrada**: 512 tokens (suficiente para enunciados do POSCOMP).\n",
        "* **Comprimento máximo de saída**: 320 tokens (para cobrir até 8 passos de raciocínio).\n",
        "\n",
        "Abaixo, mostramos um exemplo simplificado de como o treinamento pode ser feito.  \n",
        "O código completo está disponível em nosso repositório:\n",
        "\n",
        "🔗 [Agents4Good/Distillation - training_script/train.py](https://github.com/Agents4Good/Distillation/blob/training_script/train.py)\n",
        "\n",
        "```python\n",
        "def fine_tune_qlora(model_ckpt, dataset):\n",
        "  \"\"\"\n",
        "    Fine-tune model using custom training configuration and kernels\n",
        "    \"\"\"\n",
        "    device = \"cpu\"\n",
        "    if torch.accelerator.is_available():\n",
        "        device = torch.accelerator.current_accelerator()\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load student model. Save lora adapters and config\n",
        "    model = get_qlora_model(model_ckpt, liger_kernels=liger_kernels)\n",
        "    model.save_pretrained(\"./\" + model_ckpt.split(\"/\")[-1] + \"-qlora\")\n",
        "\n",
        "    # Training arguments\n",
        "    output_dir = \"distill-\" + model_ckpt.split(\"/\")[-1] + \"-poscomp\"\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "\n",
        "        num_train_epochs=3,\n",
        "        warmup_steps=0,\n",
        "        optim=\"adamw_bnb_8bit\",\n",
        "        weight_decay=0.001,\n",
        "        learning_rate=2e-4,                # Recommended in qlora paper for\n",
        "                                           # models below 33B.\n",
        "        # Data preloading\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=4,\n",
        "        remove_unused_columns=False,\n",
        "\n",
        "        # Evaluation, saving and logging\n",
        "        # Batch size of 16 and this dataset gives around 75 steps total\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=40,                    # Evaluate every n steps\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=40,                    # Save checkpoint every n steps\n",
        "        save_total_limit=2,                # Keep last 2 checkpoints\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=5,\n",
        "        logging_dir=\"./logs\",\n",
        "        report_to=\"tensorboard\",\n",
        "\n",
        "        # Optimizations\n",
        "        per_device_train_batch_size=1,\n",
        "        per_device_eval_batch_size=1,\n",
        "        gradient_accumulation_steps=16,   # Effective batch size of 16\n",
        "        gradient_checkpointing=False,     # Reduces memory usage\n",
        "                                          # Decreases speed by 20%\n",
        "        torch_compile=True,\n",
        "        torch_compile_backend=\"inductor\",\n",
        "        # torch_empty_cache_steps=4,          # Reduces memory usage.\n",
        "                                              # Decreases speed by 10%\n",
        "        group_by_length=True,\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        processing_class=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "```"
      ],
      "metadata": {
        "id": "CJpwOnjNqU1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4ª Etapa - Aplicar Budget Force**\n",
        "\n",
        "> Nesta etapa, usamos a metodologia descrita em **“s1: Simple test-time scaling”** (arXiv:2501.19393) e no relatório **“Budget forcing s1-32B: Waiting is all you need?”** do WandB.\n",
        "\n",
        "**Budget Forcing** é uma técnica utilizada durante a inferência para **controlar o uso de tokens de forma eficiente**, forçando o modelo a **priorizar raciocínios mais curtos e precisos**, especialmente útil quando se trabalha com LLMs sob restrições de custo ou tempo de resposta.\n",
        "\n",
        "### Objetivo:\n",
        "Reduzir o consumo de recursos durante a inferência, sem comprometer a qualidade da resposta.\n",
        "\n",
        "### Como funciona:\n",
        "\n",
        "- O modelo é instruído a raciocinar passo a passo, **mas com uso de um limite de tokens**.\n",
        "- Tokens especiais como `<wait>` e `<stop>` são utilizados como **sinais de controle**:\n",
        "  - ✅ **`<wait>`**: indica que o raciocínio atual está muito curto e o modelo deve **continuar pensando** antes de responder.\n",
        "  - 🛑 **`<stop>`**: indica que o raciocínio está ficando longo demais e o modelo deve **encerrar imediatamente** e retornar a melhor resposta possível até ali.\n",
        "- Esses tokens podem ser usados para:\n",
        "  - **forçar respostas dentro de um orçamento (\"budget\")**\n",
        "\n",
        "```python\n",
        "# Pseudocódigo: Aplicando Budget Force\n",
        "\n",
        "# Definições iniciais\n",
        "modelo = carregar_modelo(\"aluno\")  # Ex: qwen3-4B\n",
        "entrada = preparar_entrada(questao)  # enunciado + alternativas\n",
        "budget_tokens = 300  # Número ideal de tokens para o raciocínio\n",
        "\n",
        "# 1. Geração inicial com token <wait> no final do raciocínio-alvo\n",
        "saida_alvo = raciocinio_professor + \" <wait>\"\n",
        "\n",
        "# 2. Geração do modelo aluno com forçamento orçamentário\n",
        "resposta_aluno = modelo.gerar(\n",
        "    entrada,\n",
        "    max_tokens=budget_tokens + margem,\n",
        "    forcar_token_final=True,\n",
        "    tokens_terminadores=[\"<wait>\", \"<stop>\"]\n",
        ")\n",
        "\n",
        "# 3. Avaliação do comprimento da resposta\n",
        "if contar_tokens(resposta_aluno) < budget_tokens:\n",
        "    # Raciocínio curto demais — força o modelo a continuar pensando\n",
        "    resposta_aluno += \" <wait>\"\n",
        "    resposta_aluno += modelo.continuar_gerando(entrada + resposta_aluno)\n",
        "\n",
        "elif contar_tokens(resposta_aluno) > budget_tokens:\n",
        "    # Raciocínio longo demais — interrompe a geração\n",
        "    resposta_aluno = cortar_ate_token(resposta_aluno, \"<stop>\")\n",
        "\n",
        "# 4. Armazenamento do raciocínio ajustado\n",
        "salvar(resposta_aluno)\n",
        "```\n",
        "\n",
        "**Benefícios Constatados:**\n",
        "Segundo os experimentos do WandB, aplicando **Budget Forcing** em modelos grandes (como o s1-32B, que é baseado em Qwen2.5-32B-Instruct), obteve-se:\n",
        "\n",
        "* **Aumento de até 27% na acurácia** de raciocínios matemáticos complexos (MATH e AIME24), comparado ao modelo sem Budget Forcing.\n",
        "* **Melhora consistente** em extrapolar para problemas mais difíceis, mesmo quando o modelo já estava finamente ajustado.\n",
        "* **Economia de tokens**: em questões simples, o classificador interrompe cedo, poupando cerca de **30–40%** do orçamento de inferência, sem perder acurácia ([arXiv][1], [Weights & Biases][2])."
      ],
      "metadata": {
        "id": "1rr97EF7tCv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tarefa Futura: Acurácia do Modelo POSCOMP**\n"
      ],
      "metadata": {
        "id": "Rbjd4IaPgxd6"
      }
    }
  ]
}